{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework #6\n",
        "\n",
        "## GitHub Link: https://github.com/garcimat/EE-399/tree/main/HW6"
      ],
      "metadata": {
        "id": "TScMH7KcSPzQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG-jUO_mBZPg"
      },
      "source": [
        "### SHRED applied to SST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUvYYkGBZPi"
      },
      "source": [
        "This iPython notebook gives an introductory walkthrough to using SHRED models.  The dataset we consider is weekly mean sea-surface temperature as given by the NOAA Optimum Interpolation SST V2 dataset (https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html).\n",
        "\n",
        "SHRED (SHallow REcurrent Decoder) models are a network architecture that merges a recurrent layer (LSTM) with a shallow decoder network (SDN) to reconstruct high-dimensional spatio-temporal fields from a trajectory of sensor measurements of the field. More formally, the SHRED architecture can be written as \n",
        "$$ \\mathcal {H} \\left( \\{ y_i \\} _{i=t-k}^t \\right) = \\mathcal {F} \\left( \\mathcal {G} \\left( \\{ y_i \\} _{i=t-k}^t \\right) ; W_{RN}) ; W_{SD} \\right)$$\n",
        "where $\\mathcal F$ is a feed forward network parameterized by weights $W_{SD}$, $\\mathcal G$ is a LSTM network parameterized by weights $W_{RN}$, and $\\{ y_i \\} _{i=t-k}^t$ is a trajectory of sensor measurements of a high-dimensional spatio-temporal field $\\{ x_i \\} _{i=t-k}^t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzOO6ls0BZPi"
      },
      "source": [
        "We first randomly select 3 sensor locations and set the trajectory length (lags) to 52, corresponding to one year of measurements.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ILzA52QQBZPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from processdata import load_data\n",
        "from processdata import TimeSeriesDataset\n",
        "import models\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "num_sensors = 3 \n",
        "lags = 52\n",
        "load_X = load_data('SST')\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36KIGUvBZPk"
      },
      "source": [
        "We now select indices to divide the data into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Mj3PsOfNBZPk"
      },
      "outputs": [],
      "source": [
        "train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "mask = np.ones(n - lags)\n",
        "mask[train_indices] = 0\n",
        "valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
        "valid_indices = valid_test_indices[::2]\n",
        "test_indices = valid_test_indices[1::2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZTRqlr4BZPk"
      },
      "source": [
        "sklearn's MinMaxScaler is used to preprocess the data for training and we generate input/output pairs for the training, validation, and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JyKIiF2BBZPk"
      },
      "outputs": [],
      "source": [
        "sc = MinMaxScaler()\n",
        "sc = sc.fit(load_X[train_indices])\n",
        "transformed_X = sc.transform(load_X)\n",
        "\n",
        "### Generate input sequences to a SHRED model\n",
        "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "for i in range(len(all_data_in)):\n",
        "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
        "\n",
        "### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
        "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
        "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "\n",
        "### -1 to have output be at the same time as final sensor measurements\n",
        "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
        "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9ilEqqBZPl"
      },
      "source": [
        "We train the model using the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A-74rHoBZPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1782f17-7804-4407-be3f-0f5e08ae1281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "Error tensor(0.4812)\n",
            "Training epoch 20\n",
            "Error tensor(0.2260)\n",
            "Training epoch 40\n",
            "Error tensor(0.2176)\n",
            "Training epoch 60\n",
            "Error tensor(0.2156)\n",
            "Training epoch 80\n",
            "Error tensor(0.2147)\n",
            "Training epoch 100\n",
            "Error tensor(0.2061)\n",
            "Training epoch 120\n",
            "Error tensor(0.1978)\n"
          ]
        }
      ],
      "source": [
        "shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJhhnS3BZPl"
      },
      "source": [
        "Finally, we generate reconstructions from the test set and print mean square error compared to the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LGSEJd2BZPl"
      },
      "outputs": [],
      "source": [
        "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Train the model and plot the results"
      ],
      "metadata": {
        "id": "2z0gBmy0Sb9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of sensors and lags\n",
        "num_sensors = 3\n",
        "lags = 52\n",
        "\n",
        "# Load the data\n",
        "load_X = load_data('SST')\n",
        "\n",
        "# Get the dimensions of the data\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "\n",
        "# Randomly select sensor locations\n",
        "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "mask = np.ones(n - lags)\n",
        "mask[train_indices] = 0\n",
        "valid_test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
        "valid_indices = valid_test_indices[::2]\n",
        "test_indices = valid_test_indices[1::2]\n",
        "\n",
        "# Preprocess the data using MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "sc = sc.fit(load_X[train_indices])\n",
        "transformed_X = sc.transform(load_X)\n",
        "\n",
        "# Generate input sequences for the SHRED model\n",
        "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "for i in range(len(all_data_in)):\n",
        "    all_data_in[i] = transformed_X[i:i + lags, sensor_locations]\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
        "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
        "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "\n",
        "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "# Create datasets for training, validation, and testing\n",
        "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
        "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
        "\n",
        "# Initialize the SHRED model\n",
        "shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "\n",
        "# Train the model\n",
        "validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=5)\n",
        "\n",
        "# Plot the training and validation errors\n",
        "plt.plot(validation_errors['train'], label='Train')\n",
        "plt.plot(validation_errors['valid'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Training and Validation Errors')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Generate reconstructions from the test set\n",
        "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "8GU-sXlmSa-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analysis of the performance as a function of the time lag variable"
      ],
      "metadata": {
        "id": "Q7MUniHuSyDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of sensors\n",
        "num_sensors = 3\n",
        "\n",
        "# Define a range of time lags to test\n",
        "lags_range = [26, 39, 52, 65, 78]\n",
        "\n",
        "# Initialize lists to store the mean squared errors for each lag\n",
        "mse_values = []\n",
        "\n",
        "# Load the data\n",
        "load_X = load_data('SST')\n",
        "\n",
        "# Get the dimensions of the data\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "\n",
        "# Randomly select sensor locations\n",
        "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
        "\n",
        "for lags in lags_range:\n",
        "    # Split the data into training, validation, and test sets\n",
        "    train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "    mask = np.ones(n - lags)\n",
        "    mask[train_indices] = 0\n",
        "    valid_test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
        "    valid_indices = valid_test_indices[::2]\n",
        "    test_indices = valid_test_indices[1::2]\n",
        "\n",
        "    # Preprocess the data using MinMaxScaler\n",
        "    sc = MinMaxScaler()\n",
        "    sc = sc.fit(load_X[train_indices])\n",
        "    transformed_X = sc.transform(load_X)\n",
        "\n",
        "    # Generate input sequences for the SHRED model\n",
        "    all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "    for i in range(len(all_data_in)):\n",
        "        all_data_in[i] = transformed_X[i:i + lags, sensor_locations]\n",
        "\n",
        "    # Convert the data to PyTorch tensors\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "    test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "    # Create a dataset for testing\n",
        "    test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
        "\n",
        "    # Initialize the SHRED model\n",
        "    shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "\n",
        "    # Load pre-trained weights (optional)\n",
        "    # shred.load_state_dict(torch.load('shred_model.pt'))\n",
        "\n",
        "    # Generate reconstructions from the test set\n",
        "    test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "    test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    mse = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot the performance as a function of the time lag\n",
        "plt.plot(lags_range, mse_values, marker='o')\n",
        "plt.xlabel('Time Lag')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Performance vs. Time Lag')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "67_cIhFyS5R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Analysis of the performance as a function of noise (add Gaussian noise to data)"
      ],
      "metadata": {
        "id": "6QELTl6aS70t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of sensors and lags\n",
        "num_sensors = 3\n",
        "lags = 52\n",
        "\n",
        "# Define a range of noise levels to test\n",
        "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "# Initialize lists to store the mean squared errors for each noise level\n",
        "mse_values = []\n",
        "\n",
        "# Load the data\n",
        "load_X = load_data('SST')\n",
        "\n",
        "# Get the dimensions of the data\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "\n",
        "# Randomly select sensor locations\n",
        "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
        "\n",
        "for noise_level in noise_levels:\n",
        "    # Add Gaussian noise to the data\n",
        "    noisy_X = load_X + noise_level * np.random.randn(*load_X.shape)\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "    mask = np.ones(n - lags)\n",
        "    mask[train_indices] = 0\n",
        "    valid_test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
        "    valid_indices = valid_test_indices[::2]\n",
        "    test_indices = valid_test_indices[1::2]\n",
        "\n",
        "    # Preprocess the data using MinMaxScaler\n",
        "    sc = MinMaxScaler()\n",
        "    sc = sc.fit(noisy_X[train_indices])\n",
        "    transformed_X = sc.transform(noisy_X)\n",
        "\n",
        "    # Generate input sequences for the SHRED model\n",
        "    all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "    for i in range(len(all_data_in)):\n",
        "        all_data_in[i] = transformed_X[i:i + lags, sensor_locations]\n",
        "\n",
        "    # Convert the data to PyTorch tensors\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "    test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "    # Create a dataset for testing\n",
        "    test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
        "\n",
        "    # Initialize the SHRED model\n",
        "    shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "\n",
        "    # Load pre-trained weights (optional)\n",
        "    # shred.load_state_dict(torch.load('shred_model.pt'))\n",
        "\n",
        "    # Generate reconstructions from the test set\n",
        "    test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "    test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    mse = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot the performance as a function of the noise level\n",
        "plt.plot(noise_levels, mse_values, marker='o')\n",
        "plt.xlabel('Noise Level')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Performance vs. Noise Level')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oRsMNqp6TC75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Analysis of the performance as a function of the number of sensors"
      ],
      "metadata": {
        "id": "mclvGZ6dTJth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the time lag and define a range of number of sensors to test\n",
        "lags = 52\n",
        "num_sensors_range = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Initialize lists to store the mean squared errors for each number of sensors\n",
        "mse_values = []\n",
        "\n",
        "# Load the data\n",
        "load_X = load_data('SST')\n",
        "\n",
        "# Get the dimensions of the data\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "\n",
        "for num_sensors in num_sensors_range:\n",
        "    # Randomly select sensor locations\n",
        "    sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "    mask = np.ones(n - lags)\n",
        "    mask[train_indices] = 0\n",
        "    valid_test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
        "    valid_indices = valid_test_indices[::2]\n",
        "    test_indices = valid_test_indices[1::2]\n",
        "\n",
        "    # Preprocess the data using MinMaxScaler\n",
        "    sc = MinMaxScaler()\n",
        "    sc = sc.fit(load_X[train_indices])\n",
        "    transformed_X = sc.transform(load_X)\n",
        "\n",
        "    # Generate input sequences for the SHRED model\n",
        "    all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "    for i in range(len(all_data_in)):\n",
        "        all_data_in[i] = transformed_X[i:i + lags, sensor_locations]\n",
        "\n",
        "    # Convert the data to PyTorch tensors\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "    test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "    # Create a dataset for testing\n",
        "    test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
        "\n",
        "    # Initialize the SHRED model\n",
        "    shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "\n",
        "    # Generate reconstructions from the test set\n",
        "    test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "    test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    mse = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot the performance as a function of the number of sensors\n",
        "plt.plot(num_sensors_range, mse_values, marker='o')\n",
        "plt.xlabel('Number of Sensors')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Performance vs. Number of Sensors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NgkU0oE_TQMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}