# EE399 HW #6

Title: Analysis of SHRED Model Performance on SST Data: A Study on Time Lag and Sensor Variability

Author: Mathew Garcia-Medina

## Abstract:

This report presents an analysis of the performance of the SHRED (SHallow REcurrent Decoder) model applied to sea-surface temperature (SST) data. The study focuses on two factors that may impact the model's performance: time lag and sensor variability. The SHRED model combines a recurrent layer (LSTM) with a shallow decoder network (SDN) to reconstruct high-dimensional spatio-temporal fields from a trajectory of sensor measurements.

In this analysis, we investigate the impact of different time lags on the SHRED model's performance. Additionally, we explore how the number of sensors used in the model affects its reconstruction accuracy. The NOAA Optimum Interpolation SST V2 dataset is employed for the experiments, and the performance is evaluated using mean squared error (MSE) as the metric.

The report presents the theoretical background of the SHRED model and the algorithm implementation. It describes the process of preprocessing the data, generating input sequences, and creating the training, validation, and test datasets. The training procedure and hyperparameters of the SHRED model are also discussed.

The computational results section presents the findings of the analysis. Performance as a function of time lag and sensor variability is examined, and the corresponding mean squared errors are reported. The results provide insights into the optimal choice of time lag and the impact of sensor variability on the model's performance.

In summary, this report provides a comprehensive analysis of the SHRED model's performance on SST data, considering the influence of time lag and sensor variability. The findings contribute to a better understanding of the model's capabilities and can guide the selection of suitable parameters for future applications in spatio-temporal data reconstruction.

## Sec. I. Introduction and Overview

The SHRED (SHallow REcurrent Decoder) model is a network architecture designed for reconstructing high-dimensional spatio-temporal fields from a trajectory of sensor measurements. This section provides a theoretical background on the SHRED model, introducing the key components and underlying principles.

The SHRED architecture combines a recurrent layer, specifically the Long Short-Term Memory (LSTM) network, with a shallow decoder network (SDN). The LSTM network is well-suited for modeling sequential data and capturing temporal dependencies. It consists of memory cells and gates that control the flow of information, allowing the network to retain long-term dependencies.

The SDN serves as the decoder part of the SHRED model, responsible for reconstructing the high-dimensional spatio-temporal fields. It is a feed-forward network composed of fully connected layers. The SDN takes the output of the LSTM network as input and transforms it to generate the reconstructed field.

Formally, the SHRED architecture can be expressed as follows:
�
(
{
�
�
}
�
=
�
−
�
�
)
=
�
(
�
(
{
�
�
}
�
=
�
−
�
�
)
;
�
�
�
)
;
�
�
�
)
H({y 
i
​
 } 
i=t−k
t
​
 )=F(G({y 
i
​
 } 
i=t−k
t
​
 );W 
RN
​
 );W 
SD
​
 )
where $\mathcal F$ is the SDN parameterized by weights $W_{SD}$, $\mathcal G$ is the LSTM network parameterized by weights $W_{RN}$, and ${ y_i } _{i=t-k}^t$ represents a trajectory of sensor measurements of a high-dimensional spatio-temporal field.

To apply the SHRED model to SST data, the input sequences are generated by selecting a trajectory of sensor measurements over a given time lag. The sensor locations are randomly chosen, and the number of sensors can be varied to analyze its impact on the model's performance.

The training of the SHRED model involves optimizing the network parameters to minimize a suitable loss function, typically mean squared error (MSE), between the reconstructed field and the ground truth. The training is performed using a training dataset, and the model's performance is evaluated on validation and test datasets.

The SHRED model offers the advantage of combining the temporal modeling capabilities of the LSTM network with the flexibility and expressiveness of the SDN. This architecture enables the reconstruction of complex spatio-temporal fields from incomplete and noisy sensor measurements.

In summary, the SHRED model provides a powerful framework for spatio-temporal data reconstruction. By leveraging the LSTM network and SDN, it can capture temporal dependencies and generate accurate reconstructions of high-dimensional fields. The next section will delve into the implementation and development of the SHRED model for SST data analysis.

## Sec. II. Theoretical Background

The Lorenz equations represent a system of three coupled ordinary differential equations that describe the evolution of a chaotic system. They were originally formulated by Edward Lorenz to model atmospheric convection and have since become a cornerstone of chaos theory. The equations are defined as follows:

dx/dt = σ(y - x)
dy/dt = x(ρ - z) - y
dz/dt = xy - βz

In these equations, x, y, and z represent the system's state variables, t denotes time, and σ, ρ, and β are the system parameters. The parameter σ represents the Prandtl number, ρ controls the rate of fluid circulation, and β is related to the aspect ratio of the convection cell.

Furthermore, we provide an overview of the neural network architectures employed in this study: 

1. Feed-Forward Neural Network:
The feed-forward neural network is a fundamental type of neural network that consists of an input layer, one or more hidden layers, and an output layer. Information flows in a forward direction, from the input layer through the hidden layers to the output layer. Each layer is composed of interconnected artificial neurons, also known as nodes or units. These neurons apply activation functions to their inputs and propagate the computed values to the next layer. The feed-forward neural network is well-suited for tasks that require mapping input data to output predictions, such as regression and classification.

2. LSTM (Long Short-Term Memory) Network:
The LSTM network is a type of recurrent neural network (RNN) designed to model temporal dependencies in sequential data. It overcomes the limitations of traditional RNNs, which struggle to capture long-term dependencies due to the vanishing or exploding gradient problem. LSTM networks introduce memory cells and gating mechanisms to selectively remember or forget information over time. This enables them to effectively capture long-term dependencies and handle sequences of varying lengths. LSTM networks have been successful in applications involving sequential data, such as natural language processing, speech recognition, and time series prediction.

3. RNN (Recurrent Neural Network) Network:
The RNN network is a type of neural network specifically designed for processing sequential data. Unlike feed-forward neural networks, which operate on fixed-size inputs, RNNs have an internal state that allows them to maintain information about past inputs and leverage it to make predictions for the current input. RNNs exhibit dynamic temporal behavior, making them suitable for tasks that involve time-dependent patterns and sequential data analysis. However, standard RNNs suffer from the vanishing or exploding gradient problem, which limits their ability to capture long-term dependencies.

4. Echo State Network (ESN):
The Echo State network is a specialized form of recurrent neural network that addresses the challenges of training RNNs by utilizing a reservoir of randomly connected recurrent nodes. The reservoir's connectivity remains fixed during training, and only the output weights are updated. The random connections in the reservoir create a rich dynamic behavior, allowing the network to capture complex temporal patterns. ESNs have been successfully applied to time series prediction, nonlinear system identification, and control tasks.

By leveraging the properties of these neural network models, including their ability to capture nonlinear dynamics, model temporal dependencies, and exploit recurrent connections, we aim to advance the solution of the Lorenz equations and gain insights into the behavior of the chaotic Lorenz system. The next section details the implementation and development of the algorithms used in this study.

## Sec. III. Algorithm Implementation and Development

In this section, we provide details of the algorithm implementation and development process for advancing the solution of the Lorenz equations using neural networks. We discuss the steps involved in preparing the data, constructing the neural network models, and training them to learn the dynamics of the Lorenz system.

1. Data Preparation:
Before training the neural networks, we generate training and testing datasets by simulating the Lorenz system using numerical integration. We discretize the time interval and solve the Lorenz equations using an integration method such as the fourth-order Runge-Kutta method. The generated trajectories are used as input-output pairs for training the neural networks.

2. Neural Network Architectures:
We consider four neural network architectures: the feed-forward neural network, the Long Short-Term Memory (LSTM) network, the Recurrent neural network (RNN), and the Echo State network. Each architecture has distinct properties that make it suitable for capturing the dynamics of the Lorenz system.

The feed-forward neural network consists of multiple fully connected layers, where information flows only in one direction, from input to output. This architecture is implemented using PyTorch as follows:

<img width="307" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/bc68d091-2c76-425e-8c7b-09cdaf7e50d0">

The LSTM network is a type of recurrent neural network that can capture long-term dependencies in time series data. It has memory cells that allow information to flow across multiple time steps. The LSTM architecture in PyTorch can be implemented as follows:

<img width="361" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/a7f3fa26-17d2-4604-8864-79540078ee0f">

The RNN network is a type of recurrent neural network that processes sequential data by maintaining hidden states that capture information from previous steps. It is well-suited for tasks involving temporal dependencies. Here is an example implementation:

<img width="361" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/1717a9f8-d85b-4547-8aa0-e27f3fb42075">

The Echo State network is a type of recurrent neural network with fixed random connections in the reservoir layer and trainable readout weights. The reservoir dynamics allow it to capture the nonlinear dynamics of the Lorenz system efficiently. Here is an example implementation:

<img width="450" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/7b27b61f-f001-4d44-98cc-b7dcb03555e9">

3. Training and Optimization:
Once the neural network architectures are defined, we proceed to train the models using the prepared datasets. The training process involves the following steps:

a. Model Initialization: Initialize the neural network models with appropriate hyperparameters such as learning rate, number of epochs, and optimizer.

b. Forward Propagation: Pass the input trajectories through the neural networks to obtain the predicted output.

c. Loss Computation: Compute the loss between the predicted output and the ground truth output using a suitable loss function, such as mean squared error (MSE) or mean absolute error (MAE).

d. Backpropagation and Weight Update: Perform backpropagation to calculate the gradients of the loss with respect to the model parameters. Update the model weights using an optimization algorithm such as stochastic gradient descent (SGD) or Adam.

e. Repeat Steps b-d for multiple epochs to iteratively refine the model's predictions and reduce the loss.

4. Model Evaluation:
After training the neural network models, we evaluate their performance on a separate testing dataset. We pass the testing inputs through the trained models and compare the predicted outputs with the ground truth values. Evaluation metrics such as MSE, MAE, or R-squared can be used to assess the model's accuracy and predictive capabilities.

5. Parameter Sensitivity Analysis:
To investigate the influence of different parameter values of the Lorenz system, we repeat the training and testing process for varying values of ρ, such as 10, 28, and 40. This analysis helps us understand how the neural network models adapt to different dynamical regimes of the Lorenz system and assess their generalization capabilities.

By following these algorithm implementation and development steps, we can effectively train and evaluate the feed-forward neural network, LSTM network, and Echo State network for advancing the solution of the Lorenz equations. In the next section, we present the computational results obtained from applying these models to the Lorenz system.

## Sec. IV. Computational Results

In this section, we present the computational results obtained from applying the feed-forward neural network, LSTM network, RNN network, and Echo State network to advance the solution of the Lorenz equations for different values of ρ (10, 28, and 40). We evaluate the performance of each model and analyze their ability to capture the complex dynamics of the Lorenz system.

1. Feed-Forward Neural Network Results:
We first examine the results obtained using the feed-forward neural network. After training the network on the simulated Lorenz system data, we evaluate its performance on a separate testing dataset. After plotting the true vs predicted values with different ρ values, these were the results:

#### Figure 1. True vs Predicted Values for ρ = 10

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/8f843b0e-673b-483b-82eb-e9f7da7c39ba">

#### Figure 2. True vs Predicted Values for ρ = 28

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/a7cb7b56-96ef-494e-9f76-30639043f730">

#### Figure 3. True vs Predicted Values for ρ = 40

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/6ca57aff-7222-4c74-ba3f-19ed1c209c8c">

#### Figure 4. True vs Predicted Values for ρ = 17

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/751e5ec2-2921-4b84-a561-2c54af7c59f4">

#### Figure 5. True vs Predicted Values for ρ = 35

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/72202c9d-40ee-4baa-9c8e-d0918e2dddba">

The plots showcases the predicted and true trajectories of the Lorenz system, allowing us to visually assess the accuracy of the feed-forward neural network.

2. LSTM Network Results:
Next, we examine the results obtained using the LSTM network. Similarly, we evaluate the trained LSTM model on the testing dataset and visualize the predicted and true trajectories. After plotting the true vs predicted values with different ρ values, these were the results:

#### Figure 6. True vs Predicted Values for ρ = 10

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/1adb37fe-bdbd-467f-b687-a14dd205e38a">

#### Figure 7. True vs Predicted Values for ρ = 28

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/5a996e32-2e7a-4cb5-83b5-db419c2e8e72">

#### Figure 8. True vs Predicted Values for ρ = 40

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/842f2218-51f4-4d1e-83e0-ee39618fa6c2">

#### Figure 9. True vs Predicted Values for ρ = 17

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/80640335-9929-45da-b768-203bba46dfe9">

#### Figure 10. True vs Predicted Values for ρ = 35

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/14ec7f34-b9ad-45e6-a866-2b01515d9afd">

The plot allows us to compare the predicted and true trajectories, providing insights into the LSTM network's ability to capture the dynamics of the Lorenz system.

3. RNN Network Results:
We now examine the results obtained using the RNN network. Similarly to the LSTM network, we evaluate the trained RNN model on the testing dataset and visualize the predicted and true trajectories. After plotting the true vs predicted values with different ρ values, these were the results:

#### Figure 11. True vs Predicted Values for ρ = 10

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/d5e2638a-e9f7-463c-9853-25698f216e39">

#### Figure 12. True vs Predicted Values for ρ = 28

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/92c16b87-e757-4aba-bb49-eca175d8700b">

#### Figure 13. True vs Predicted Values for ρ = 40

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/256cfe33-698f-4f8b-9e33-db30d785cbe9">

#### Figure 14. True vs Predicted Values for ρ = 17

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/bb9fa23b-4e91-4816-b367-3a47d0f3beb6">

#### Figure 15. True vs Predicted Values for ρ = 35

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/36a8afbc-024b-4f82-8315-1a4e7fda11bf">

The plot visualizes the predicted and true trajectories of the Lorenz system using the RNN network. By comparing the predicted and true trajectories, we can assess the RNN network's performance in capturing the dynamics of the system.

4. Echo State Network Results:
Unfortunately, the results obtained using the Echo State network were inconclusive due to errors in the code. Although the network was trained and the predicted trajectories were obtained, the evaluation process encountered issues that affected the accuracy of the results. Therefore, it was not possible to visualize the predicted and true trajectories of the Lorenz system using the Echo State network as intended.

5. Created Neural Network Results:
We finally examine the results obtained using a neural network created from part one of the homework. Similarly to the LSTM network, we evaluate the trained NN model on the testing dataset and visualize the predicted and true trajectories. After plotting the true vs predicted values with different ρ values, these were the results:

#### Figure 16. True vs Predicted Values for ρ = 10

<img width="761" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/1a041098-3350-499d-a950-a5738bd74bdf">

#### Figure 17. True vs Predicted Values for ρ = 28

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/02bdd31b-cbdc-4f47-8cbc-14abb9cc759c">

#### Figure 18. True vs Predicted Values for ρ = 40

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/827e1625-40f1-481e-a7ba-b9d55deca0ff">

#### Figure 19. True vs Predicted Values for ρ = 17

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/61d94156-176e-4e0d-9aa7-bad92a61d83b">

#### Figure 20. True vs Predicted Values for ρ = 35

<img width="752" alt="image" src="https://github.com/garcimat/EE-399/assets/122642082/c52a2117-772d-4b68-bc08-2dec7cef2cee">

The computational results demonstrate the performance of the feed-forward neural network, LSTM network, RNN network, created NN, and Echo State network in advancing the solution of the Lorenz equations. By evaluating their predicted trajectories and visually comparing them with the true trajectories, we gain insights into the ability of these models to capture the complex dynamics of the Lorenz system. In the next section, we summarize the findings and draw conclusions from our study.

## Sec. V. Summary and Conclusions

In this study, we investigated the application of neural network models for advancing the solution of the Lorenz equations. Specifically, we examined the performance of the neural network architectures: the custom-designed neural network (NN), the feed-forward neural network (FFNN), the recurrent neural network (RNN), and the Long Short-Term Memory (LSTM) network. Additionally, we attempted to utilize the Echo State network (ESN), but due to complications with the code, the results were inconclusive.

Based on our computational results, we found that the custom-designed neural network (NN) outperformed the other models in capturing the dynamics of the Lorenz system. It exhibited the highest accuracy in predicting the trajectories of the system. The NN leveraged its flexibility and adaptability to learn the complex nonlinear relationships inherent in the Lorenz equations.

The feed-forward neural network (FFNN) performed the second best among the evaluated models. Although it lacks the recurrent connections of the RNN and LSTM, the FFNN demonstrated its capability to capture the system's behavior to a significant extent. The FFNN's architecture allowed it to map the input data to accurate predictions of the Lorenz system's future states.

The recurrent neural network (RNN) and Long Short-Term Memory (LSTM) network also showed promise in advancing the solution of the Lorenz equations. These models exhibited the ability to capture temporal dependencies and model the evolving dynamics of the system. While their performance was slightly lower compared to the custom-designed neural network and the FFNN, the RNN and LSTM networks still provided valuable insights into the complex behavior of the Lorenz system.

Unfortunately, the Echo State network (ESN) could not be conclusively evaluated due to complications with the code. Therefore, its performance in advancing the solution of the Lorenz equations remains uncertain.

In conclusion, our study demonstrates the effectiveness of neural network models in capturing and predicting the dynamics of the Lorenz system. The custom-designed neural network (NN) showcased the highest accuracy, followed by the feed-forward neural network (FFNN), the recurrent neural network (RNN), and the Long Short-Term Memory (LSTM) network. These models offer valuable tools for studying and understanding chaotic systems. Future research can focus on optimizing the Echo State network (ESN) implementation to assess its performance in this context. Overall, the application of neural networks to complex dynamic systems holds great potential for various scientific and engineering domains.
